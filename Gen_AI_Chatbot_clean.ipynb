{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "i62sIEbRJnCw",
    "outputId": "e21584f8-a792-4f62-dd6e-88f125e24661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
      "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
      "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.7.0)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
      "Requirement already satisfied: langchain_chroma in /usr/local/lib/python3.11/dist-packages (0.2.4)\n",
      "Collecting langchain_groq\n",
      "  Downloading langchain_groq-0.3.6-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.68)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.10.1)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.4)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.4.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.1)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.22.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.35.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.73.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.24.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
      "Collecting groq<1,>=0.29.0 (from langchain_groq)\n",
      "  Downloading groq-0.30.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
      "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain_groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain_groq) (1.9.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain_groq) (1.3.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.7.9)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.35.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.35.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.56b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.56b0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Downloading langchain_groq-0.3.6-py3-none-any.whl (16 kB)\n",
      "Downloading groq-0.30.0-py3-none-any.whl (131 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: groq, langchain_groq\n",
      "Successfully installed groq-0.30.0 langchain_groq-0.3.6\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain_community langchain chromadb pypdf langchain sentence-transformers langchain_chroma langchain_groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5scn81DmtaPK"
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import os\n",
    "import warnings\n",
    "import json\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tc-DxeNT1itd"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "ff39400a7e3b4528b837df39e2253aeb",
      "c0e1337e0f524578a114ccc75170442d",
      "41ef4890d03540d1856679ba569bb21c",
      "11916d4324f4443e9f7c8b18d7afcb0a",
      "6ac2f9881d7143b88037fd72f4090a08",
      "74967290bf334fefa3372d5e1e6f4ebd",
      "56bc382f918b4733a1ca1df70b399049",
      "2837209ba9814a81aa5da051ac1d615a",
      "a7f74e3f5c1645a4846cc3813910ea71",
      "fce900fe8f0246f8ad2bfc27cf3e9ade",
      "bef24ff7a8114353ad63dda27bf907c9",
      "f0dbda7f609740499a4aa4cd91010b90",
      "f15314645aff45b7afcdc90263881ae4",
      "c3f54526e5504fd19cfe7ca8b85a65ea",
      "dcb69ae7ddab4a24a31ee861f35b8513",
      "7bd1c21ab48a46aa81f58f0d0012e037",
      "1e28fddd07ef4ca6a498490e7261cbaa",
      "a6f961d670de42e0b5b79d3dd68e5e6d",
      "244d992de85e4a6185988e2222571f75",
      "6b5a79e1b6ed4a188b3db49c1386a859",
      "1df415763c094545b5c4025749d5a6db",
      "c2e4d991d49f426c881677f038b83e76",
      "3d76754570734516a833bd395325bdc9",
      "78435978d2b9432593a9098595994b02",
      "c80981b3c21f43caac1df5d0497a09d8",
      "14736d4509ed4f669d737456aea32073",
      "898ebd4dc9e54dd0872d0c2394cb160e",
      "684f3171c73a434a9ec6980612e17126",
      "61b5d12c0d9945cbb3613944cce8f604",
      "240c524d4f6347689ef6e4c0425adc48",
      "615d6cbb64ca42ac9bf84d37f0b74119",
      "435396ae246448d484c05a73e01513ba",
      "9636596bcbfb45cb85143b2f8e8954aa",
      "733896b08d4643f2839562c496588821",
      "afd704268aa44691a5172fda4347b08f",
      "ace2b999655840fea8225015ce541468",
      "06fb76275c4646768921319f5f393137",
      "a08376146db449fa9bee50450fbab7e2",
      "093816518bf343acbbfab92dab94a249",
      "4a9ed38dbb674d2f94cb1828eebaa238",
      "fce6098138f34e59a814850bffd98f89",
      "dc6aa38958bf41d597bbfbca041bfa1d",
      "abf03b15882448c696869e064d45fb52",
      "6453590138094e87b5d680b7e8e1b289",
      "6c254d1b70ac41408dd11314f58353a9",
      "d9f592c03baa4fefb4f0c77db6cdcb73",
      "94db3d493a9a4b23aa1c1c804e1840f3",
      "c71e398b79104e12b916f0aaa8389ed8",
      "d95e74fa957a478ebccbc44938fd3cac",
      "ed127a91279748909189a47e18d8d647",
      "a2d4435377b0478d818c0f520508061a",
      "11da90de500e49dc963913c3a273e115",
      "3684f876edfd41ab8fc91526be650280",
      "f64657584f364074976817f1c49a034b",
      "789b0bed028d4bf7ae30a0f82ff7784f",
      "7dbe0c5eace94e0e8e6f52fdf07b21a5",
      "2311872f6623410494ca066db701b68a",
      "0966128347bd43f7bcbc9c6a6f91aca5",
      "cbb7697bd2004637aae0a13c7421e4e6",
      "c2ef14c155904eb7a2505d9ec0538934",
      "b4b4469c6bdb474a8cbbb94d9931f3a6",
      "00fb8629d23a43698c578e27c362ec85",
      "18f20ae211e64f7d8b80aea57fec2ec3",
      "cf4ee000a03a48418cd2367213a7c8f8",
      "15067b8e32c8450280f6c28e4e7c2b56",
      "ed6a514307f94360acdb1b52fd4d99a9",
      "18e9ba0fff9b4cd3a0c3e6ba094a606b",
      "13312bec026f4a27ac14af540c79e6d2",
      "851c49415e2441709e868828605082bf",
      "fb0cee778a014312824d6bf8f3f2806a",
      "379a54585d5a4938b593a837f6c8ac3f",
      "56dd58eb025443a9bad5d9f750f6df7f",
      "369d606d966f4b82854b8044e4e350ec",
      "5b8bb7098faf4ba2a01a950f8f467a2e",
      "aef7a457231c4c63bf3abe83d72b6cde",
      "2331a1a2b8974cc2b01fc30e8b49e62c",
      "59d64a98ab9a41c1b8235f2d3d8093e2",
      "8684d05bb4ae4f40a7c48cd6d02e2945",
      "fe5b126d49a742f4935af0e0b77935cf",
      "1170d77ebf9149fab8163f7841b7a560",
      "fd1aa47eb99c4a9fac5bb52c6f0b0423",
      "7e138d6a80e54f029fba39730c90776f",
      "a5d87d5147784aa7b41184c96807541c",
      "bec17b479c614bd49e4197d57cdedcd7",
      "dd0de8a298654a119571698c68aa2c63",
      "e331d53da0844a99b5d5c03d7a89e779",
      "a328c68e723240ba9e667e935b67e391",
      "095be28a11d84011847be94cae650b45",
      "609d7d7ddd07444ca0dfca4baefedf2d",
      "3a66c29bbdc049a9852377da8faace1c",
      "ef94b02fb9054b5b8baf02637311ec1a",
      "7e2734b0e47a4c4c9b089205b44900be",
      "937fa23ca3fb48ad803d5eed03b3aff9",
      "52d6fd4121dd47adba6b7c646bfcdaee",
      "01147cf4ca924f62be5710eeb658971b",
      "0d657d6cd27a4fc08f2d374b98c5dac6",
      "6cf62339bce34fb7bd2f7a5db1375fbc",
      "e0a369bea2c14714bea271eebbe1d13a",
      "5ad4ac7be5d24930873936290b37b914",
      "6e52a8c0928e4b879585788262fb5f9f",
      "6513ec88b99f4c8c9ca62a263ace5e8d",
      "7eda9b9ce8084fc6aa8f8215fead8f17",
      "e7ff065650f24dff978b97d2451b2ff3",
      "283ceb95bbf14ddd8fc7abe6485d09c2",
      "a497ae1ef178444b97bc7c6fd8daaea8",
      "fa1e9c3cd8cf4ea1abcfd176b19a6860",
      "ff4038609c48416fbf80ae03e11aee18",
      "cec0dcbf05c8494cabbd0f778744bc6e",
      "1a6166ad15b44f10b22f4efecf6ee728",
      "fc2871106e214a99b9886bd156963ff8",
      "166e4543c34640e89d457af8142c5535",
      "53923283907f42f7af6993b1a8464405",
      "381b8b2536fd4aa0a1a56075cd03326a",
      "c52e95b48d424bd49902ef6c14e9c3c5",
      "005ba98ddade46f69748679d6bae1c0b",
      "5018a803c1b049fd92affa63b5913120",
      "7178ba9fd703418385e0e2a62bc1343e",
      "14a6a8b02ab24f9f9fa1e5da0c62dbd7",
      "d59861637b5b477eb03da0394fc6379f",
      "ee0fba8dad0e4fd7b9be7a1de196486a",
      "2dff718e72c34b35852afe426e8df293"
     ]
    },
    "id": "zawKpARg0BTO",
    "outputId": "f315e994-7092-4c1f-d875-47bc7afcda24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 ENHANCED DATA INGESTION FOR RAG CHATBOT\n",
      "============================================================\n",
      "🔧 SETTING UP GOOGLE COLAB ENVIRONMENT...\n",
      "==================================================\n",
      "✅ Created data directory: /content/data\n",
      "✅ Found 1 supported files:\n",
      "   • Build_Dont_Talk_.pdf (.pdf)\n",
      "\n",
      "🔄 Starting ingestion process...\n",
      "🔧 ENHANCED DATA INGESTION FOR RAG SYSTEM\n",
      "==================================================\n",
      "📁 Data path: /content/data\n",
      "🗄️  Chroma path: /content/chroma\n",
      "✂️  Chunk size: 1000\n",
      "🔄 Chunk overlap: 200\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff39400a7e3b4528b837df39e2253aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0dbda7f609740499a4aa4cd91010b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d76754570734516a833bd395325bdc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733896b08d4643f2839562c496588821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c254d1b70ac41408dd11314f58353a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dbe0c5eace94e0e8e6f52fdf07b21a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e9ba0fff9b4cd3a0c3e6ba094a606b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8684d05bb4ae4f40a7c48cd6d02e2945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609d7d7ddd07444ca0dfca4baefedf2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e52a8c0928e4b879585788262fb5f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166e4543c34640e89d457af8142c5535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-12 14:03:58] INFO: 🚀 Starting enhanced data ingestion pipeline...\n",
      "[2025-07-12 14:03:58] INFO: Found 1 supported files\n",
      "[2025-07-12 14:03:58] INFO: Processing: Build_Dont_Talk_.pdf\n",
      "[2025-07-12 14:03:58] INFO: Loading PDF file: /content/data/Build_Dont_Talk_.pdf\n",
      "[2025-07-12 14:04:00] INFO: Loaded 178 pages from Build_Dont_Talk_.pdf\n",
      "[2025-07-12 14:04:00] INFO: Total documents loaded: 178\n",
      "[2025-07-12 14:04:00] INFO: Splitting 178 documents into chunks...\n",
      "[2025-07-12 14:04:00] INFO: Created 406 chunks\n",
      "[2025-07-12 14:04:00] INFO: Average chunk size: 768 characters\n",
      "[2025-07-12 14:04:00] INFO: Adding unique IDs to chunks...\n",
      "[2025-07-12 14:04:00] INFO: Added IDs to 406 chunks\n",
      "[2025-07-12 14:04:00] INFO: Preparing to save to ChromaDB...\n",
      "[2025-07-12 14:04:00] INFO: Testing ChromaDB connection...\n",
      "[2025-07-12 14:04:01] INFO: ChromaDB connection test passed!\n",
      "[2025-07-12 14:04:01] INFO: Creating new ChromaDB database...\n",
      "[2025-07-12 14:04:27] INFO: Verifying database creation...\n",
      "[2025-07-12 14:04:28] INFO: Database verification: 406 documents stored\n",
      "[2025-07-12 14:04:28] INFO: ✅ Successfully saved 406 chunks to ChromaDB\n",
      "[2025-07-12 14:04:28] INFO: ✅ Database location: /content/chroma\n",
      "\n",
      "============================================================\n",
      "🎉 DATA INGESTION COMPLETE!\n",
      "============================================================\n",
      "📊 STATISTICS:\n",
      "   • Documents processed: 178\n",
      "   • Chunks created: 406\n",
      "   • Files processed: 1\n",
      "   • File types: {'pdf': 178}\n",
      "   • Total content: 276,057 characters\n",
      "   • Average chunk size: 768 characters\n",
      "   • Database path: /content/chroma\n",
      "============================================================\n",
      "📁 PROCESSED FILES:\n",
      "   • Build_Dont_Talk_.pdf\n",
      "============================================================\n",
      "📋 PROCESSING LOG (last 5 entries):\n",
      "   [2025-07-12 14:04:01] INFO: Creating new ChromaDB database...\n",
      "   [2025-07-12 14:04:27] INFO: Verifying database creation...\n",
      "   [2025-07-12 14:04:28] INFO: Database verification: 406 documents stored\n",
      "   [2025-07-12 14:04:28] INFO: ✅ Successfully saved 406 chunks to ChromaDB\n",
      "   [2025-07-12 14:04:28] INFO: ✅ Database location: /content/chroma\n",
      "============================================================\n",
      "[2025-07-12 14:04:28] INFO: ✅ Data ingestion pipeline completed successfully!\n",
      "\n",
      "🎯 NEXT STEPS:\n",
      "1. Your ChromaDB is ready for the RAG chatbot!\n",
      "2. Run the chatbot cells to start asking questions\n",
      "3. The chatbot will automatically use this database\n",
      "\n",
      "✅ Ready to chat with Harry!\n",
      "\n",
      "🔍 Verifying compatibility...\n",
      "🔍 VERIFYING DATABASE COMPATIBILITY...\n",
      "==================================================\n",
      "✅ Embedding function loaded successfully\n",
      "✅ ChromaDB loaded successfully\n",
      "✅ Search test passed - found 1 results\n",
      "✅ Search with score test passed - found 1 results\n",
      "✅ Database contains 406 documents\n",
      "==================================================\n",
      "🎉 DATABASE IS FULLY COMPATIBLE WITH THE CHATBOT!\n",
      "==================================================\n",
      "\n",
      "🎯 SUMMARY:\n",
      "Your ChromaDB is ready for the RAG chatbot system!\n",
      "The database is fully compatible and aligned with the chatbot.\n"
     ]
    }
   ],
   "source": [
    "CHROMA_PATH = \"/content/chroma\"  # Same path as chatbot\n",
    "DATA_PATH = \"/content/data\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "def get_embedding_function():\n",
    "    \"\"\"\n",
    "    Create and return an embedding function - IDENTICAL to chatbot version.\n",
    "    \"\"\"\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "    model_name = \"all-MiniLM-L6-v2\"\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': False}\n",
    "    )\n",
    "\n",
    "    # embeddings = HuggingFaceEmbeddings(\n",
    "    #     model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    #     model_kwargs={'device': 'cpu'},\n",
    "    #     encode_kwargs={'normalize_embeddings': True}\n",
    "    # )\n",
    "\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "class AlignedDataIngestionPipeline:\n",
    "    \"\"\"\n",
    "    Data ingestion pipeline aligned with the Enhanced RAG Chatbot System.\n",
    "    Supports: Text files and PDFs with enhanced error handling and logging.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str = DATA_PATH, chroma_path: str = CHROMA_PATH):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.chroma_path = chroma_path\n",
    "        self.embedding_function = get_embedding_function()\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=CHUNK_SIZE,\n",
    "            chunk_overlap=CHUNK_OVERLAP,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "        self.supported_extensions = {'.txt', '.pdf'}\n",
    "        self.processing_log = []\n",
    "\n",
    "    def log_processing(self, message: str, level: str = \"INFO\"):\n",
    "        \"\"\"Enhanced logging for processing steps\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_entry = f\"[{timestamp}] {level}: {message}\"\n",
    "        self.processing_log.append(log_entry)\n",
    "        print(log_entry)\n",
    "\n",
    "    def load_text_file(self, file_path: Path) -> List[Document]:\n",
    "        \"\"\"Load text files with enhanced error handling\"\"\"\n",
    "        try:\n",
    "            self.log_processing(f\"Loading text file: {file_path}\")\n",
    "\n",
    "            # Try multiple encodings\n",
    "            encodings = ['utf-8', 'latin-1', 'cp1252']\n",
    "            documents = None\n",
    "\n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    loader = TextLoader(str(file_path), encoding=encoding)\n",
    "                    documents = loader.load()\n",
    "                    self.log_processing(f\"Successfully loaded with {encoding} encoding\")\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "\n",
    "            if documents is None:\n",
    "                raise Exception(\"Could not load file with any encoding\")\n",
    "\n",
    "            # Enhanced metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata.update({\n",
    "                    'source': str(file_path),\n",
    "                    'type': 'text',\n",
    "                    'file_name': file_path.name,\n",
    "                    'file_size': file_path.stat().st_size,\n",
    "                    'processed_at': datetime.now().isoformat()\n",
    "                })\n",
    "\n",
    "            self.log_processing(f\"Loaded {len(documents)} documents from {file_path.name}\")\n",
    "            return documents\n",
    "\n",
    "        except Exception as e:\n",
    "            self.log_processing(f\"Error loading text file {file_path}: {e}\", \"ERROR\")\n",
    "            return []\n",
    "\n",
    "    def load_pdf_file(self, file_path: Path) -> List[Document]:\n",
    "        \"\"\"Load PDF files with enhanced error handling\"\"\"\n",
    "        try:\n",
    "            self.log_processing(f\"Loading PDF file: {file_path}\")\n",
    "\n",
    "            loader = PyPDFLoader(str(file_path))\n",
    "            documents = loader.load()\n",
    "\n",
    "            # Enhanced metadata\n",
    "            for i, doc in enumerate(documents):\n",
    "                doc.metadata.update({\n",
    "                    'source': str(file_path),\n",
    "                    'type': 'pdf',\n",
    "                    'file_name': file_path.name,\n",
    "                    'page_number': i + 1,\n",
    "                    'total_pages': len(documents),\n",
    "                    'file_size': file_path.stat().st_size,\n",
    "                    'processed_at': datetime.now().isoformat()\n",
    "                })\n",
    "\n",
    "            self.log_processing(f\"Loaded {len(documents)} pages from {file_path.name}\")\n",
    "            return documents\n",
    "\n",
    "        except Exception as e:\n",
    "            self.log_processing(f\"Error loading PDF file {file_path}: {e}\", \"ERROR\")\n",
    "            return []\n",
    "\n",
    "    def load_documents_from_directory(self) -> List[Document]:\n",
    "        \"\"\"Load all supported documents from the data directory\"\"\"\n",
    "        documents = []\n",
    "\n",
    "        # Create data directory if it doesn't exist\n",
    "        self.data_path.mkdir(exist_ok=True)\n",
    "\n",
    "        # Check if directory is empty\n",
    "        files = list(self.data_path.rglob('*'))\n",
    "        supported_files = [f for f in files if f.is_file() and f.suffix.lower() in self.supported_extensions]\n",
    "\n",
    "        if not supported_files:\n",
    "            self.log_processing(\"No supported files found in data directory\", \"WARNING\")\n",
    "            self.log_processing(f\"Looking for files in: {self.data_path.absolute()}\")\n",
    "            self.log_processing(f\"Supported extensions: {self.supported_extensions}\")\n",
    "            return []\n",
    "\n",
    "        self.log_processing(f\"Found {len(supported_files)} supported files\")\n",
    "\n",
    "        for file_path in supported_files:\n",
    "            self.log_processing(f\"Processing: {file_path.name}\")\n",
    "\n",
    "            if file_path.suffix.lower() == '.txt':\n",
    "                docs = self.load_text_file(file_path)\n",
    "            elif file_path.suffix.lower() == '.pdf':\n",
    "                docs = self.load_pdf_file(file_path)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            documents.extend(docs)\n",
    "\n",
    "        self.log_processing(f\"Total documents loaded: {len(documents)}\")\n",
    "        return documents\n",
    "\n",
    "    def split_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"Split documents into chunks with enhanced logging\"\"\"\n",
    "        self.log_processing(f\"Splitting {len(documents)} documents into chunks...\")\n",
    "\n",
    "        chunks = self.text_splitter.split_documents(documents)\n",
    "\n",
    "        # Add chunk statistics\n",
    "        chunk_sizes = [len(chunk.page_content) for chunk in chunks]\n",
    "        avg_chunk_size = sum(chunk_sizes) / len(chunk_sizes) if chunk_sizes else 0\n",
    "\n",
    "        self.log_processing(f\"Created {len(chunks)} chunks\")\n",
    "        self.log_processing(f\"Average chunk size: {avg_chunk_size:.0f} characters\")\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def add_chunk_ids(self, chunks: List[Document]) -> List[Document]:\n",
    "        \"\"\"Add unique IDs to chunks with enhanced metadata\"\"\"\n",
    "        self.log_processing(\"Adding unique IDs to chunks...\")\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            source = chunk.metadata.get('source', 'unknown')\n",
    "            file_name = chunk.metadata.get('file_name', Path(source).stem)\n",
    "\n",
    "            # Create more descriptive ID\n",
    "            chunk_id = f\"{file_name}_{i}\"\n",
    "            chunk.metadata['id'] = chunk_id\n",
    "            chunk.metadata['chunk_index'] = i\n",
    "            chunk.metadata['chunk_size'] = len(chunk.page_content)\n",
    "\n",
    "        self.log_processing(f\"Added IDs to {len(chunks)} chunks\")\n",
    "        return chunks\n",
    "\n",
    "    def test_database_connection(self) -> bool:\n",
    "        \"\"\"Test ChromaDB connection before saving\"\"\"\n",
    "        try:\n",
    "            self.log_processing(\"Testing ChromaDB connection...\")\n",
    "\n",
    "            # Create a test document\n",
    "            test_doc = Document(\n",
    "                page_content=\"This is a test document for ChromaDB connection.\",\n",
    "                metadata={'source': 'test', 'type': 'test'}\n",
    "            )\n",
    "\n",
    "            # Try to create a temporary ChromaDB instance\n",
    "            temp_path = f\"{self.chroma_path}_test\"\n",
    "            if os.path.exists(temp_path):\n",
    "                shutil.rmtree(temp_path)\n",
    "\n",
    "            test_db = Chroma.from_documents(\n",
    "                [test_doc],\n",
    "                self.embedding_function,\n",
    "                persist_directory=temp_path\n",
    "            )\n",
    "\n",
    "            # Test search\n",
    "            results = test_db.similarity_search(\"test\", k=1)\n",
    "\n",
    "            # Cleanup\n",
    "            shutil.rmtree(temp_path)\n",
    "\n",
    "            self.log_processing(\"ChromaDB connection test passed!\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            self.log_processing(f\"ChromaDB connection test failed: {e}\", \"ERROR\")\n",
    "            return False\n",
    "\n",
    "    def save_to_chroma(self, chunks: List[Document]):\n",
    "        \"\"\"Save chunks to ChromaDB with enhanced error handling\"\"\"\n",
    "        try:\n",
    "            self.log_processing(\"Preparing to save to ChromaDB...\")\n",
    "\n",
    "            # Test connection first\n",
    "            if not self.test_database_connection():\n",
    "                raise Exception(\"ChromaDB connection test failed\")\n",
    "\n",
    "            # Clear existing database\n",
    "            if os.path.exists(self.chroma_path):\n",
    "                self.log_processing(\"Clearing existing database...\")\n",
    "                shutil.rmtree(self.chroma_path)\n",
    "\n",
    "            # Create new database\n",
    "            self.log_processing(\"Creating new ChromaDB database...\")\n",
    "\n",
    "            db = Chroma.from_documents(\n",
    "                chunks,\n",
    "                self.embedding_function,\n",
    "                persist_directory=self.chroma_path\n",
    "            )\n",
    "\n",
    "            # Verify database creation\n",
    "            self.log_processing(\"Verifying database creation...\")\n",
    "            test_results = db.similarity_search(\"test\", k=1)\n",
    "\n",
    "            # Get database statistics\n",
    "            try:\n",
    "                collection = db._collection\n",
    "                doc_count = collection.count()\n",
    "                self.log_processing(f\"Database verification: {doc_count} documents stored\")\n",
    "            except:\n",
    "                self.log_processing(\"Database created successfully (count verification unavailable)\")\n",
    "\n",
    "            self.log_processing(f\"✅ Successfully saved {len(chunks)} chunks to ChromaDB\")\n",
    "            self.log_processing(f\"✅ Database location: {os.path.abspath(self.chroma_path)}\")\n",
    "\n",
    "            return db\n",
    "\n",
    "        except Exception as e:\n",
    "            self.log_processing(f\"Error saving to ChromaDB: {e}\", \"ERROR\")\n",
    "            raise\n",
    "\n",
    "    def show_enhanced_summary(self, documents: List[Document], chunks: List[Document]):\n",
    "        \"\"\"Show enhanced processing summary\"\"\"\n",
    "        file_types = {}\n",
    "        processed_files = set()\n",
    "        total_content_length = 0\n",
    "\n",
    "        for doc in documents:\n",
    "            doc_type = doc.metadata.get('type', 'unknown')\n",
    "            source = doc.metadata.get('source', 'unknown')\n",
    "            file_name = doc.metadata.get('file_name', Path(source).name)\n",
    "\n",
    "            file_types[doc_type] = file_types.get(doc_type, 0) + 1\n",
    "            processed_files.add(file_name)\n",
    "            total_content_length += len(doc.page_content)\n",
    "\n",
    "        chunk_sizes = [len(chunk.page_content) for chunk in chunks]\n",
    "        avg_chunk_size = sum(chunk_sizes) / len(chunk_sizes) if chunk_sizes else 0\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"🎉 DATA INGESTION COMPLETE!\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"📊 STATISTICS:\")\n",
    "        print(f\"   • Documents processed: {len(documents)}\")\n",
    "        print(f\"   • Chunks created: {len(chunks)}\")\n",
    "        print(f\"   • Files processed: {len(processed_files)}\")\n",
    "        print(f\"   • File types: {dict(file_types)}\")\n",
    "        print(f\"   • Total content: {total_content_length:,} characters\")\n",
    "        print(f\"   • Average chunk size: {avg_chunk_size:.0f} characters\")\n",
    "        print(f\"   • Database path: {os.path.abspath(self.chroma_path)}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"📁 PROCESSED FILES:\")\n",
    "        for file_name in sorted(processed_files):\n",
    "            print(f\"   • {file_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Show some processing log entries\n",
    "        print(f\"📋 PROCESSING LOG (last 5 entries):\")\n",
    "        for log_entry in self.processing_log[-5:]:\n",
    "            print(f\"   {log_entry}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "    def process_all_data(self):\n",
    "        \"\"\"Main processing pipeline with enhanced error handling\"\"\"\n",
    "        try:\n",
    "            self.log_processing(\"🚀 Starting enhanced data ingestion pipeline...\")\n",
    "\n",
    "            # Load documents\n",
    "            documents = self.load_documents_from_directory()\n",
    "            if not documents:\n",
    "                self.log_processing(\"❌ No documents found to process!\", \"ERROR\")\n",
    "                return False\n",
    "\n",
    "            # Split into chunks\n",
    "            chunks = self.split_documents(documents)\n",
    "            if not chunks:\n",
    "                self.log_processing(\"❌ No chunks created!\", \"ERROR\")\n",
    "                return False\n",
    "\n",
    "            # Add IDs and save\n",
    "            chunks = self.add_chunk_ids(chunks)\n",
    "            db = self.save_to_chroma(chunks)\n",
    "\n",
    "            # Show summary\n",
    "            self.show_enhanced_summary(documents, chunks)\n",
    "\n",
    "            self.log_processing(\"✅ Data ingestion pipeline completed successfully!\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            self.log_processing(f\"❌ Pipeline failed: {e}\", \"ERROR\")\n",
    "            return False\n",
    "\n",
    "def run_enhanced_ingestion(data_path=DATA_PATH, chroma_path=CHROMA_PATH,\n",
    "                          chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP):\n",
    "    \"\"\"\n",
    "    Run the enhanced data ingestion pipeline with full compatibility.\n",
    "    \"\"\"\n",
    "    print(\"🔧 ENHANCED DATA INGESTION FOR RAG SYSTEM\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"📁 Data path: {data_path}\")\n",
    "    print(f\"🗄️  Chroma path: {chroma_path}\")\n",
    "    print(f\"✂️  Chunk size: {chunk_size}\")\n",
    "    print(f\"🔄 Chunk overlap: {chunk_overlap}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    pipeline = AlignedDataIngestionPipeline(data_path=data_path, chroma_path=chroma_path)\n",
    "\n",
    "    # Update chunk settings if different from defaults\n",
    "    if chunk_size != CHUNK_SIZE or chunk_overlap != CHUNK_OVERLAP:\n",
    "        pipeline.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "        pipeline.log_processing(f\"Using custom chunk settings: size={chunk_size}, overlap={chunk_overlap}\")\n",
    "\n",
    "    success = pipeline.process_all_data()\n",
    "\n",
    "    if success:\n",
    "        print(\"\\n🎯 NEXT STEPS:\")\n",
    "        print(\"1. Your ChromaDB is ready for the RAG chatbot!\")\n",
    "        print(\"2. Run the chatbot cells to start asking questions\")\n",
    "        print(\"3. The chatbot will automatically use this database\")\n",
    "        print(\"\\n✅ Ready to chat with Harry!\")\n",
    "    else:\n",
    "        print(\"\\n❌ Ingestion failed. Check the logs above for details.\")\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "def verify_database_compatibility():\n",
    "    \"\"\"\n",
    "    Verify that the created database is compatible with the chatbot.\n",
    "    \"\"\"\n",
    "    print(\"🔍 VERIFYING DATABASE COMPATIBILITY...\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    try:\n",
    "        # Test embedding function compatibility\n",
    "        embedding_function = get_embedding_function()\n",
    "        print(\"✅ Embedding function loaded successfully\")\n",
    "\n",
    "        # Test ChromaDB loading\n",
    "        if not os.path.exists(CHROMA_PATH):\n",
    "            print(\"❌ ChromaDB not found. Run ingestion first.\")\n",
    "            return False\n",
    "\n",
    "        db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "        print(\"✅ ChromaDB loaded successfully\")\n",
    "\n",
    "        # Test search functionality\n",
    "        test_results = db.similarity_search(\"test query\", k=1)\n",
    "        print(f\"✅ Search test passed - found {len(test_results)} results\")\n",
    "\n",
    "        # Test with score\n",
    "        results_with_score = db.similarity_search_with_score(\"test query\", k=1)\n",
    "        print(f\"✅ Search with score test passed - found {len(results_with_score)} results\")\n",
    "\n",
    "        # Show database info\n",
    "        try:\n",
    "            collection = db._collection\n",
    "            doc_count = collection.count()\n",
    "            print(f\"✅ Database contains {doc_count} documents\")\n",
    "        except:\n",
    "            print(\"✅ Database is accessible (document count unavailable)\")\n",
    "\n",
    "        print(\"=\"*50)\n",
    "        print(\"🎉 DATABASE IS FULLY COMPATIBLE WITH THE CHATBOT!\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Compatibility test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def setup_colab_environment():\n",
    "    \"\"\"\n",
    "    Set up the Google Colab environment for data ingestion.\n",
    "    \"\"\"\n",
    "    print(\"🔧 SETTING UP GOOGLE COLAB ENVIRONMENT...\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Create data directory\n",
    "    data_dir = Path(DATA_PATH)\n",
    "    data_dir.mkdir(exist_ok=True)\n",
    "    print(f\"✅ Created data directory: {data_dir.absolute()}\")\n",
    "\n",
    "    # Check if files exist\n",
    "    files = list(data_dir.rglob('*'))\n",
    "    supported_files = [f for f in files if f.is_file() and f.suffix.lower() in {'.txt', '.pdf'}]\n",
    "\n",
    "    if not supported_files:\n",
    "        print(\"📋 UPLOAD INSTRUCTIONS:\")\n",
    "        print(\"1. Upload your .txt or .pdf files to the 'data' folder\")\n",
    "        print(\"2. You can drag and drop files in the Colab file browser\")\n",
    "        print(\"3. Or use the upload button in the file browser\")\n",
    "        print(\"4. Supported formats: .txt, .pdf\")\n",
    "        print()\n",
    "        print(\"🔍 To upload files programmatically:\")\n",
    "        print(\"   from google.colab import files\")\n",
    "        print(\"   uploaded = files.upload()\")\n",
    "        print(\"   # Then move files to the data folder\")\n",
    "\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"✅ Found {len(supported_files)} supported files:\")\n",
    "        for file_path in supported_files:\n",
    "            print(f\"   • {file_path.name} ({file_path.suffix})\")\n",
    "\n",
    "        return True\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the complete ingestion process.\n",
    "    \"\"\"\n",
    "    print(\"🚀 ENHANCED DATA INGESTION FOR RAG CHATBOT\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    has_files = setup_colab_environment()\n",
    "\n",
    "    if not has_files:\n",
    "        print(\"\\n⚠️  Please upload your documents first!\")\n",
    "        return\n",
    "\n",
    "    # Run ingestion\n",
    "    print(\"\\n🔄 Starting ingestion process...\")\n",
    "    pipeline = run_enhanced_ingestion()\n",
    "\n",
    "    # Verify compatibility\n",
    "    print(\"\\n🔍 Verifying compatibility...\")\n",
    "    verify_database_compatibility()\n",
    "\n",
    "    print(\"\\n🎯 SUMMARY:\")\n",
    "    print(\"Your ChromaDB is ready for the RAG chatbot system!\")\n",
    "    print(\"The database is fully compatible and aligned with the chatbot.\")\n",
    "\n",
    "\n",
    "# Uncomment to test the system\n",
    "# print(\"🧪 Testing the aligned system...\")\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rg3FILMhxqMU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XajKnZRSNVqh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-I_th052n5yN"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def get_embedding_function():\n",
    "    \"\"\"\n",
    "    Returns embedding function using all-mpnet-base-v2 model.\n",
    "\n",
    "    Returns:\n",
    "        HuggingFaceEmbeddings: Configured embedding function\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def test_embedding_function(embedding_func, test_phrases=None):\n",
    "    \"\"\"Test the embedding function with sample phrases\"\"\"\n",
    "\n",
    "    if test_phrases is None:\n",
    "        test_phrases = [\n",
    "            \"artificial intelligence\",\n",
    "            \"machine learning algorithms\",\n",
    "            \"natural language processing\",\n",
    "            \"deep learning models\"\n",
    "        ]\n",
    "\n",
    "    print(\"Testing embedding function...\")\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "    try:\n",
    "        for i, phrase in enumerate(test_phrases):\n",
    "            embedding = embedding_func.embed_query(phrase)\n",
    "            print(f\"{i+1}. '{phrase}' → Vector dim: {len(embedding)}\")\n",
    "\n",
    "        print(\"✅ Embedding function test completed successfully!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Embedding function test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Main execution\n",
    "print(\"🚀 Loading all-mpnet-base-v2 embedding model...\")\n",
    "\n",
    "try:\n",
    "    embedding_func = get_embedding_function()\n",
    "    print(\"✅ Embedding model loaded successfully!\")\n",
    "\n",
    "    # Test the function\n",
    "    test_embedding_function(embedding_func)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading embedding model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HeyY3g8poxwu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WDLnUOcwox46"
   },
   "outputs": [],
   "source": [
    "# Cell 3: Configuration\n",
    "CHROMA_PATH = \"/content/chroma\"\n",
    "\n",
    "CONVERSATIONAL_ANSWER_TEMPLATE = \"\"\"\n",
    "You are Harry, a helpful, knowledgeable, and friendly AI technical support assistant.\n",
    "Your goal is to answer questions about technical documentation accurately and helpfully.\n",
    "\n",
    "You have access to relevant context from the documents and the conversation history.\n",
    "Use both the context and chat history to provide comprehensive, contextual answers.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Always be conversational and remember what was discussed before\n",
    "2. Reference previous parts of the conversation when relevant\n",
    "3. If asked for clarification, elaborate on previous responses\n",
    "4. Provide well-cited, accurate responses\n",
    "5. If greeting, be friendly and offer help\n",
    "6. For follow-up questions, build upon previous context\n",
    "7. If user is asking irrelevant questions just polietly say \"I don't have an information about that but I can help you with the motivational textbook named as 'Build Don't Talk'\"\n",
    "\n",
    "Context from Documents: {context}\n",
    "\n",
    "Chat History: {chat_history}\n",
    "\n",
    "Current Question: {question}\n",
    "\n",
    "Harry's Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "cU7X0uthox8N"
   },
   "outputs": [],
   "source": [
    "NO_CONTEXT_TEMPLATE = \"\"\"\n",
    "You are Harry, a helpful technical support assistant.\n",
    "\n",
    "I don't have specific context from the documents for this question, but I can help based on our conversation history.\n",
    "\n",
    "Chat History: {chat_history}\n",
    "\n",
    "Current Question: {question}\n",
    "\n",
    "Harry's Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "RnW84-P_3mdp"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "btt5DyG8381M",
    "outputId": "8db37503-c6b7-4d3d-ee6a-ee9bdc6e3373"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGroq model initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "model = ChatGroq(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        temperature=0.4,  # Slightly higher for more conversational responses\n",
    "        max_tokens=4096,\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "    )\n",
    "print(\"ChatGroq model initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "zsPqK19K388D"
   },
   "outputs": [],
   "source": [
    "def get_embedding_function():\n",
    "    \"\"\"\n",
    "    Create and return an embedding function.\n",
    "    \"\"\"\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "    model_name = \"all-MiniLM-L6-v2\"\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': False}\n",
    "    )\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "X0Z5r8YH38_a"
   },
   "outputs": [],
   "source": [
    "class ConversationalMemoryManager:\n",
    "    \"\"\"\n",
    "    Manages conversation history and memory for the RAG system.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_memory_length=10):\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True,\n",
    "            max_token_limit=2000\n",
    "        )\n",
    "        self.max_memory_length = max_memory_length\n",
    "        self.conversation_log = []\n",
    "\n",
    "    def add_message(self, human_message, ai_message):\n",
    "        \"\"\"Add a new message pair to memory\"\"\"\n",
    "        self.memory.chat_memory.add_user_message(human_message)\n",
    "        self.memory.chat_memory.add_ai_message(ai_message)\n",
    "\n",
    "        # Log the conversation with timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        self.conversation_log.append({\n",
    "            \"timestamp\": timestamp,\n",
    "            \"human\": human_message,\n",
    "            \"ai\": ai_message\n",
    "        })\n",
    "\n",
    "        # Keep only recent conversations to prevent memory overflow\n",
    "        if len(self.conversation_log) > self.max_memory_length:\n",
    "            self.conversation_log.pop(0)\n",
    "\n",
    "    def get_chat_history(self):\n",
    "        \"\"\"Get formatted chat history\"\"\"\n",
    "        history = self.memory.chat_memory.messages\n",
    "        formatted_history = []\n",
    "\n",
    "        for msg in history:\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                formatted_history.append(f\"Human: {msg.content}\")\n",
    "            elif isinstance(msg, AIMessage):\n",
    "                formatted_history.append(f\"Harry: {msg.content}\")\n",
    "\n",
    "        return \"\\n\".join(formatted_history)\n",
    "\n",
    "    def get_recent_context(self, num_exchanges=3):\n",
    "        \"\"\"Get recent conversation context\"\"\"\n",
    "        recent_log = self.conversation_log[-num_exchanges:] if self.conversation_log else []\n",
    "        context = []\n",
    "\n",
    "        for exchange in recent_log:\n",
    "            context.append(f\"Human: {exchange['human']}\")\n",
    "            context.append(f\"Harry: {exchange['ai']}\")\n",
    "\n",
    "        return \"\\n\".join(context)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clear all conversation memory\"\"\"\n",
    "        self.memory.clear()\n",
    "        self.conversation_log.clear()\n",
    "\n",
    "    def get_conversation_summary(self):\n",
    "        \"\"\"Get a summary of the current conversation\"\"\"\n",
    "        if not self.conversation_log:\n",
    "            return \"No previous conversation.\"\n",
    "\n",
    "        total_exchanges = len(self.conversation_log)\n",
    "        recent_topics = []\n",
    "\n",
    "        for exchange in self.conversation_log[-3:]:  # Last 3 exchanges\n",
    "            recent_topics.append(exchange['human'][:50] + \"...\" if len(exchange['human']) > 50 else exchange['human'])\n",
    "\n",
    "        return f\"Conversation has {total_exchanges} exchanges. Recent topics: {', '.join(recent_topics)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "agKkNZqA4R-_"
   },
   "outputs": [],
   "source": [
    "memory_manager = ConversationalMemoryManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "HtFUB0PV4SBB"
   },
   "outputs": [],
   "source": [
    "def query_rag_with_memory(query_text: str, use_memory=True):\n",
    "    \"\"\"\n",
    "    Query the RAG system with conversational memory support.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): The user's question\n",
    "        use_memory (bool): Whether to use conversation history\n",
    "\n",
    "    Returns:\n",
    "        str: The answer from the RAG system\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize embedding function and database\n",
    "        embedding_function = get_embedding_function()\n",
    "        db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "        # Get chat history\n",
    "        chat_history = memory_manager.get_chat_history() if use_memory else \"\"\n",
    "\n",
    "        # Handle greetings\n",
    "        greetings = ['hi', 'hello', 'hey', 'greetings', 'good morning', 'good afternoon', 'good evening']\n",
    "        if query_text.lower().strip() in greetings:\n",
    "            response = f\"Hello! I am here to assist you with the 'Build Don't Talk' Book, ask me your question {memory_manager.get_conversation_summary()}\"\n",
    "            if use_memory:\n",
    "                memory_manager.add_message(query_text, response)\n",
    "            return response\n",
    "\n",
    "        # Search the database for relevant context\n",
    "        results = db.similarity_search_with_score(query_text, k=5)\n",
    "\n",
    "        print(\"Retrieved contexts:\")\n",
    "        for i, (doc, score) in enumerate(results, 1):\n",
    "            print(f\"Context {i}: Score={score:.4f}, Content={doc.page_content[:100]}...\")\n",
    "\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Combine context from search results\n",
    "        context_text = \"\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "\n",
    "        # Choose template based on context availability\n",
    "        if context_text.strip():\n",
    "            prompt_template = ChatPromptTemplate.from_template(CONVERSATIONAL_ANSWER_TEMPLATE)\n",
    "            prompt = prompt_template.format(\n",
    "                context=context_text,\n",
    "                chat_history=chat_history,\n",
    "                question=query_text\n",
    "            )\n",
    "        else:\n",
    "            prompt_template = ChatPromptTemplate.from_template(NO_CONTEXT_TEMPLATE)\n",
    "            prompt = prompt_template.format(\n",
    "                chat_history=chat_history,\n",
    "                question=query_text\n",
    "            )\n",
    "\n",
    "        # Generate response using the model\n",
    "        response = model.invoke(prompt)\n",
    "\n",
    "        # Extract content from response\n",
    "        if hasattr(response, 'content'):\n",
    "            response_content = response.content\n",
    "        else:\n",
    "            response_content = str(response)\n",
    "\n",
    "        # Add to memory if enabled\n",
    "        if use_memory:\n",
    "            memory_manager.add_message(query_text, response_content)\n",
    "\n",
    "        return response_content\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing query: {str(e)}\"\n",
    "        if use_memory:\n",
    "            memory_manager.add_message(query_text, error_msg)\n",
    "        return error_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "q6-6ub3E4SC-"
   },
   "outputs": [],
   "source": [
    "def ask_harry(question, remember=True):\n",
    "    \"\"\"\n",
    "    Ask Harry a question with optional memory.\n",
    "\n",
    "    Args:\n",
    "        question (str): Your question for Harry\n",
    "        remember (bool): Whether to remember this conversation\n",
    "\n",
    "    Returns:\n",
    "        str: Harry's response\n",
    "    \"\"\"\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    response = query_rag_with_memory(question, use_memory=remember)\n",
    "    print(f\"Harry's Answer: {response}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "OEJX6CuK9ffa",
    "outputId": "e95cfe2f-fa7c-4212-9aa8-ef9a5a1fa8a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what the author is saying\n",
      "==================================================\n",
      "Retrieved contexts:\n",
      "Context 1: Score=1.1694, Content=Fact: Most people don’t achieve anything in their life because most people\n",
      "don’t do anything in thei...\n",
      "Context 2: Score=1.2283, Content=‘Raj, your course is very good. All the things that you have done and learnt\n",
      "in the last five to six...\n",
      "Context 3: Score=1.3273, Content=the reason I’m becoming a creator and expanding my reach, so that I can be\n",
      "a crucial part of the Ind...\n",
      "Context 4: Score=1.3391, Content=Preface: Little Accomplishments\n",
      "I hate reading books, but I’m writing one.\n",
      "If you love reading books...\n",
      "Context 5: Score=1.3444, Content=scenario, like, you know what, if you buy my book today then your life can\n",
      "also get better, like min...\n",
      "Harry's Answer: Hello. I'm happy to help you understand what the author is saying. The author appears to be emphasizing the importance of taking action and making progress in small, manageable steps. They're encouraging readers to focus on achieving \"little accomplishments\" rather than feeling overwhelmed by large goals or projects.\n",
      "\n",
      "The author is also being quite honest about their own approach to learning and personal growth, admitting that they don't enjoy reading books but are writing one anyway. They're offering alternative ways for people to learn, such as through videos or podcasts, and emphasizing that the key to growth is through consistent, daily efforts.\n",
      "\n",
      "It seems that the author is trying to motivate readers to take control of their own learning and development, rather than relying on traditional methods or procrastinating. They're using their own experiences and successes as examples, and encouraging readers to apply the principles outlined in the book to improve their own lives.\n",
      "\n",
      "What specifically would you like to know or discuss about the author's message? I'm here to help you understand it better.\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Hello. I\\'m happy to help you understand what the author is saying. The author appears to be emphasizing the importance of taking action and making progress in small, manageable steps. They\\'re encouraging readers to focus on achieving \"little accomplishments\" rather than feeling overwhelmed by large goals or projects.\\n\\nThe author is also being quite honest about their own approach to learning and personal growth, admitting that they don\\'t enjoy reading books but are writing one anyway. They\\'re offering alternative ways for people to learn, such as through videos or podcasts, and emphasizing that the key to growth is through consistent, daily efforts.\\n\\nIt seems that the author is trying to motivate readers to take control of their own learning and development, rather than relying on traditional methods or procrastinating. They\\'re using their own experiences and successes as examples, and encouraging readers to apply the principles outlined in the book to improve their own lives.\\n\\nWhat specifically would you like to know or discuss about the author\\'s message? I\\'m here to help you understand it better.'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_harry('what the author is saying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "FLRJXc0Z4SE2"
   },
   "outputs": [],
   "source": [
    "def ask_harry_with_context():\n",
    "    \"\"\"\n",
    "    Ask Harry with full conversation context displayed.\n",
    "    \"\"\"\n",
    "    question = input(\"Your question: \")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CONVERSATION CONTEXT:\")\n",
    "    print(memory_manager.get_conversation_summary())\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    response = ask_harry(question)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"UPDATED CONVERSATION HISTORY:\")\n",
    "    recent_history = memory_manager.get_recent_context(3)\n",
    "    print(recent_history if recent_history else \"No conversation history yet.\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "y5d0alC84SG5"
   },
   "outputs": [],
   "source": [
    "def clear_conversation():\n",
    "    \"\"\"Clear the conversation memory.\"\"\"\n",
    "    memory_manager.clear_memory()\n",
    "    print(\"Conversation memory cleared!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "M68zsb0P4SKR"
   },
   "outputs": [],
   "source": [
    "def show_conversation_history():\n",
    "    \"\"\"Display the current conversation history.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CONVERSATION HISTORY:\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    history = memory_manager.get_chat_history()\n",
    "    if history:\n",
    "        print(history)\n",
    "    else:\n",
    "        print(\"No conversation history yet.\")\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total exchanges: {len(memory_manager.conversation_log)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "9oeyiUcp4oG7"
   },
   "outputs": [],
   "source": [
    "def create_conversational_widget_interface():\n",
    "    \"\"\"\n",
    "    Create an enhanced widget interface with conversation memory.\n",
    "    \"\"\"\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, clear_output\n",
    "\n",
    "    # Create widgets\n",
    "    question_input = widgets.Text(\n",
    "        placeholder='Ask me a question...',\n",
    "        description='Question:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    ask_button = widgets.Button(description='Ask Harry', button_style='primary', layout=widgets.Layout(width='15%'))\n",
    "    eli5_button = widgets.Button(description='ELI5 Mode', button_style='info', layout=widgets.Layout(width='15%'))\n",
    "    clear_button = widgets.Button(description='Clear Chat', button_style='warning', layout=widgets.Layout(width='10%'))\n",
    "    history_button = widgets.Button(description='Show History', button_style='success', layout=widgets.Layout(width='10%'))\n",
    "\n",
    "    output_area = widgets.Output()\n",
    "\n",
    "    # Ask Handler (used by both button and Enter key)\n",
    "    def on_ask_click(b=None):\n",
    "        with output_area:\n",
    "            output_area.clear_output()\n",
    "            query = question_input.value.strip()\n",
    "            if query and len(query) >= 3:\n",
    "                print(f\"🤔 You: {query}\")\n",
    "                print(\"-\" * 50)\n",
    "                response = query_rag_with_memory(query)\n",
    "                print(f\"🤖 Harry: {response}\")\n",
    "                print(\"=\" * 50)\n",
    "                question_input.value = \"\"\n",
    "            else:\n",
    "                print(\"❌ Please enter a valid question (min 3 characters).\")\n",
    "\n",
    "    def on_eli5_click(b):\n",
    "        with output_area:\n",
    "            output_area.clear_output()\n",
    "            query = question_input.value.strip()\n",
    "            if query:\n",
    "                print(f\"🤔 You (ELI5): {query}\")\n",
    "                print(\"-\" * 50)\n",
    "                response = explain_like_im_five(query)\n",
    "                print(f\"🤖 Harry (ELI5): {response}\")\n",
    "                print(\"=\" * 50)\n",
    "                question_input.value = \"\"\n",
    "            else:\n",
    "                print(\"❌ Please enter a question for ELI5 mode!\")\n",
    "\n",
    "    def on_clear_click(b):\n",
    "        clear_conversation()\n",
    "        output_area.clear_output()\n",
    "        print(\"🧹 Conversation cleared!\")\n",
    "\n",
    "    def on_history_click(b):\n",
    "        with output_area:\n",
    "            output_area.clear_output()\n",
    "            print(\"📜 CONVERSATION HISTORY:\")\n",
    "            print(\"=\" * 60)\n",
    "            history = memory_manager.get_chat_history()\n",
    "            if history:\n",
    "                print(history)\n",
    "            else:\n",
    "                print(\"No conversation history yet.\")\n",
    "            print(\"=\" * 60)\n",
    "\n",
    "    # Connect events\n",
    "    ask_button.on_click(on_ask_click)\n",
    "    eli5_button.on_click(on_eli5_click)\n",
    "    clear_button.on_click(on_clear_click)\n",
    "    history_button.on_click(on_history_click)\n",
    "\n",
    "    # ✅ Submit on Enter\n",
    "    question_input.on_submit(on_ask_click)\n",
    "\n",
    "    # Display interface\n",
    "    print(\"🤖 Harry's Enhanced Technical Support with Memory\")\n",
    "    print(\"=\" * 60)\n",
    "    display(widgets.VBox([\n",
    "        question_input,\n",
    "        widgets.HBox([ask_button, eli5_button, clear_button, history_button]),\n",
    "        output_area\n",
    "    ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "tLEXXv_H4oI9"
   },
   "outputs": [],
   "source": [
    "def interactive_mode_with_memory():\n",
    "    \"\"\"\n",
    "    Run an enhanced interactive session with memory capabilities.\n",
    "    \"\"\"\n",
    "    print(\"🤖 Welcome to Harry's Enhanced Technical Support!\")\n",
    "    print(\"Features: Conversational Memory, ELI5 Mode, Summarization\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Commands:\")\n",
    "    print(\"- 'quit' or 'exit' - End session\")\n",
    "    print(\"- 'eli5 <question>' - Explain like I'm 5\")\n",
    "    print(\"- 'history' - Show conversation history\")\n",
    "    print(\"- 'clear' - Clear conversation memory\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            question = input(f\"\\n[{len(memory_manager.conversation_log)} exchanges] Your question: \")\n",
    "\n",
    "            if question.lower() in ['quit', 'exit', 'bye']:\n",
    "                print(\"Thank you for using Harry's Technical Support!\")\n",
    "\n",
    "                # Offer to save conversation\n",
    "                save_choice = input(\"Would you like to save this conversation? (y/n): \")\n",
    "                if save_choice.lower() == 'y':\n",
    "                    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                    filename = f\"conversation_{timestamp}.json\"\n",
    "\n",
    "                    conversation_data = {\n",
    "                        \"timestamp\": timestamp,\n",
    "                        \"total_exchanges\": len(memory_manager.conversation_log),\n",
    "                        \"conversation\": memory_manager.conversation_log\n",
    "                    }\n",
    "\n",
    "                    with open(filename, 'w') as f:\n",
    "                        json.dump(conversation_data, f, indent=2)\n",
    "\n",
    "                    print(f\"Conversation saved to {filename}\")\n",
    "\n",
    "                break\n",
    "\n",
    "            elif question.lower() == 'history':\n",
    "                show_conversation_history()\n",
    "\n",
    "            elif question.lower() == 'clear':\n",
    "                clear_conversation()\n",
    "\n",
    "            elif question.lower().startswith('eli5 '):\n",
    "                eli5_question = question[5:]  # Remove 'eli5 ' prefix\n",
    "                print(f\"\\n🎈 ELI5 Mode - Question: {eli5_question}\")\n",
    "                print(\"-\" * 40)\n",
    "                response = explain_like_im_five(eli5_question)\n",
    "                print(f\"Harry (ELI5): {response}\")\n",
    "\n",
    "            elif question.strip():\n",
    "                print(f\"\\n💬 Processing: {question}\")\n",
    "                print(\"-\" * 40)\n",
    "                response = ask_harry(question)\n",
    "\n",
    "            else:\n",
    "                print(\"Please enter a valid question or command.\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nSession interrupted.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "uXIaVtSG4oLO"
   },
   "outputs": [],
   "source": [
    "def export_conversation(filename=None):\n",
    "    \"\"\"\n",
    "    Export conversation to JSON file.\n",
    "    \"\"\"\n",
    "    if not filename:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"conversation_{timestamp}.json\"\n",
    "\n",
    "    conversation_data = {\n",
    "        \"export_timestamp\": datetime.now().isoformat(),\n",
    "        \"total_exchanges\": len(memory_manager.conversation_log),\n",
    "        \"conversation\": memory_manager.conversation_log,\n",
    "    }\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(conversation_data, f, indent=2)\n",
    "\n",
    "    print(f\"Conversation exported to {filename}\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "ps34KgqU4oOo"
   },
   "outputs": [],
   "source": [
    "def import_conversation(filename):\n",
    "    \"\"\"\n",
    "    Import conversation from JSON file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            conversation_data = json.load(f)\n",
    "\n",
    "        # Clear current memory\n",
    "        memory_manager.clear_memory()\n",
    "\n",
    "        # Import conversation\n",
    "        for exchange in conversation_data['conversation']:\n",
    "            memory_manager.add_message(exchange['human'], exchange['ai'])\n",
    "\n",
    "        print(f\"Conversation imported from {filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error importing conversation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "XVjYwII-46Pr"
   },
   "outputs": [],
   "source": [
    "def launch_enhanced_widget_interface():\n",
    "    \"\"\"Launch the enhanced IPython widgets interface\"\"\"\n",
    "    print(\"Launching enhanced interface with conversational memory...\")\n",
    "    try:\n",
    "        import ipywidgets as widgets\n",
    "        from IPython.display import display\n",
    "        create_conversational_widget_interface()\n",
    "    except ImportError:\n",
    "        print(\"Installing ipywidgets...\")\n",
    "        !pip install ipywidgets\n",
    "        import ipywidgets as widgets\n",
    "        from IPython.display import display\n",
    "        create_conversational_widget_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "DS53lue-46Ry"
   },
   "outputs": [],
   "source": [
    "interactive_mode_with_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "FAMV_OKrLzOh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdSjVg-5E4tI"
   },
   "outputs": [],
   "source": [
    "launch_enhanced_widget_interface()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
